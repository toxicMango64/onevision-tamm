{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":97741,"databundleVersionId":11965955,"sourceType":"competition"}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport timm\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchmetrics\n\nfrom tqdm import tqdm\nfrom torchvision.transforms import v2\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision.models import efficientnet_b7, EfficientNet_B7_Weights, resnet50, EfficientNet_B7_Weights\nfrom PIL import Image\n\nsns.set(style=\"whitegrid\", palette=\"Set2\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-23T05:42:10.362259Z","iopub.execute_input":"2025-04-23T05:42:10.362586Z","iopub.status.idle":"2025-04-23T05:42:26.717782Z","shell.execute_reply.started":"2025-04-23T05:42:10.362563Z","shell.execute_reply":"2025-04-23T05:42:26.717111Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# Try SMOTE? (Not ideal, it sucks)\n# Try Cost-Sensitive Learning \n# Get from Original Dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T05:42:26.719075Z","iopub.execute_input":"2025-04-23T05:42:26.719548Z","iopub.status.idle":"2025-04-23T05:42:26.723596Z","shell.execute_reply.started":"2025-04-23T05:42:26.719515Z","shell.execute_reply":"2025-04-23T05:42:26.722742Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"PATH = \"/kaggle/input/tammathon-task-2\"\n\ntrain_df = pd.read_csv(f\"{PATH}/train.csv\")\nvalid_df = pd.read_csv(f\"{PATH}/val.csv\")\ntest_df = pd.read_csv(f\"{PATH}/test.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T05:42:26.724635Z","iopub.execute_input":"2025-04-23T05:42:26.725042Z","iopub.status.idle":"2025-04-23T05:42:26.777084Z","shell.execute_reply.started":"2025-04-23T05:42:26.725010Z","shell.execute_reply":"2025-04-23T05:42:26.776374Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"train_df.iloc[0]['path'].split('/')[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T05:42:26.778561Z","iopub.execute_input":"2025-04-23T05:42:26.779390Z","iopub.status.idle":"2025-04-23T05:42:26.791487Z","shell.execute_reply.started":"2025-04-23T05:42:26.779364Z","shell.execute_reply":"2025-04-23T05:42:26.790596Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"'train'"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"class ICAODataset(Dataset):\n    def __init__(self, root_dir, dataframe, transforms=None):\n        self.root_dir = root_dir\n        self.dataframe = dataframe.reset_index(drop=True)\n        self.transforms = transforms\n        self.subset = dataframe.iloc[0]['path'].split('/')[0]\n\n    def __len__(self):\n        return len(self.dataframe)\n\n    def __getitem__(self, idx):\n        row = self.dataframe.iloc[idx]\n        img_path = os.path.join(self.root_dir, self.subset, row['path'])\n        img = Image.open(img_path).convert(\"RGB\")  # ensure 3 channels\n        \n        label = torch.tensor(row['label'], dtype=torch.float32)  # binary case\n        if self.transforms:\n            img = self.transforms(img)\n        return img, torch.tensor([label])\n\n\ntrain_transforms = v2.Compose([\n    v2.Resize(600),\n    v2.RandomHorizontalFlip(p=0.3),\n    v2.ColorJitter(0.4, 0.4, 0.4, 0.4),\n    v2.ToImage(),\n    v2.ToDtype(torch.float32, scale=True),\n    v2.Normalize(mean=[0.485, 0.456, 0.406], \n                 std=[0.229, 0.224, 0.225])\n])\n\nvalid_transforms = v2.Compose([\n    v2.Resize(600),\n    v2.ToImage(),\n    v2.ToDtype(torch.float32, scale=True),\n    v2.Normalize(mean=[0.485, 0.456, 0.406], \n                 std=[0.229, 0.224, 0.225])\n])\n\ntrain_data = ICAODataset(PATH, train_df, transforms=train_transforms)\nvalid_data = ICAODataset(PATH, valid_df, transforms=valid_transforms)\n\ntrain_loader = DataLoader(train_data, num_workers=0, shuffle=True, \n                          pin_memory=True)\nvalid_loader = DataLoader(valid_data, num_workers=0, shuffle=False, \n                          pin_memory=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T05:49:54.409006Z","iopub.execute_input":"2025-04-23T05:49:54.409311Z","iopub.status.idle":"2025-04-23T05:49:54.421164Z","shell.execute_reply.started":"2025-04-23T05:49:54.409289Z","shell.execute_reply":"2025-04-23T05:49:54.420149Z"}},"outputs":[],"execution_count":49},{"cell_type":"code","source":"class EffNetB7Base(nn.Module):\n    def __init__(self, n_classes=1):\n        super().__init__()\n        model = efficientnet_b7(weights=EfficientNet_B7_Weights.IMAGENET1K_V1)\n        in_features = model.classifier[1].in_features\n        \n        self.backbone = nn.Sequential(*list(model.children())[:-1])\n        self.pool = nn.AdaptiveAvgPool2d(1)\n        self.classifier = nn.Linear(in_features, 1)\n\n    def forward(self, x):\n        out = self.backbone(x)\n        out = self.pool(out)\n        out = torch.flatten(out, 1)\n        out = self.classifier(out)\n        return out  # raw logits\n\n\nclass ResNet50Base(nn.Module):\n    def __init__(self, n_classes=1):\n        super().__init__()\n        model = resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)\n        in_features = model.fc.in_features\n\n        # Remove the original classifier\n        self.backbone = nn.Sequential(*list(model.children())[:-1])\n        self.classifier = nn.Linear(in_features, n_classes)\n\n    def forward(self, x):\n        out = self.backbone(x)\n        out = torch.flatten(out, 1)\n        out = self.classifier(out)\n        return out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T05:49:56.505304Z","iopub.execute_input":"2025-04-23T05:49:56.506094Z","iopub.status.idle":"2025-04-23T05:49:56.517020Z","shell.execute_reply.started":"2025-04-23T05:49:56.506061Z","shell.execute_reply":"2025-04-23T05:49:56.515920Z"}},"outputs":[],"execution_count":50},{"cell_type":"code","source":"class FocalLoss(nn.Module):\n    def __init__(self, alpha=0.25, gamma=2.0):\n        super(FocalLoss, self).__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n\n    def forward(self, inputs, targets):\n        probs = torch.sigmoid(inputs)\n        ce_loss = F.binary_cross_entropy(probs, targets, reduction='none')\n        pt = torch.exp(-ce_loss)  # pt = e^(-BCE)\n        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n        \n        return focal_loss.mean()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T05:50:03.895293Z","iopub.execute_input":"2025-04-23T05:50:03.896122Z","iopub.status.idle":"2025-04-23T05:50:03.902066Z","shell.execute_reply.started":"2025-04-23T05:50:03.896095Z","shell.execute_reply":"2025-04-23T05:50:03.900948Z"}},"outputs":[],"execution_count":51},{"cell_type":"code","source":"imgs, labels = next(iter(train_loader))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T05:50:31.097061Z","iopub.execute_input":"2025-04-23T05:50:31.097453Z","iopub.status.idle":"2025-04-23T05:50:31.191797Z","shell.execute_reply.started":"2025-04-23T05:50:31.097423Z","shell.execute_reply":"2025-04-23T05:50:31.190887Z"}},"outputs":[],"execution_count":52},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nconfig = {\n    \"lr\": 1e-4,\n    \"weight_decay\": 1e-4\n}\n\neffnet_model = EffNetB7Base(1).to(device)\nclass_weights = torch.tensor([0.67, 0.33]).to(device)\n\nfocal_loss = FocalLoss(alpha=0.75)\nbce_loss = nn.BCEWithLogitsLoss(pos_weight=class_weights)\n\noptimizer = torch.optim.AdamW(effnet_model.parameters(), \n                              lr=config['lr'],\n                              weight_decay=config['weight_decay'])\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"max\", patience=2) # So LR doesn't plateau smh\nmetric_fn = torchmetrics.F1Score(task='binary')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T05:50:34.794948Z","iopub.execute_input":"2025-04-23T05:50:34.795275Z","iopub.status.idle":"2025-04-23T05:50:36.463822Z","shell.execute_reply.started":"2025-04-23T05:50:34.795252Z","shell.execute_reply":"2025-04-23T05:50:36.463020Z"}},"outputs":[],"execution_count":54},{"cell_type":"code","source":"# For Reference: a CrossValidator class I created back then\n\nclass CrossValidator:\n    def __init__(self, \n                 models, \n                 metric_fns, \n                 cv_method, \n                 scaler = None, \n                 name = None, \n                 pi_kwargs = None, \n                 pred_probs = False, \n                 verbose = True):\n        \"\"\"\n           A class for performing cross-validation on a set of models with various metric functions.\n\n           Attributes:\n               models (list): A list of tuples containing the model name and the model object.\n               metric_fns (list): A list of tuples containing the metric name and the metric function.\n               cv_method (object): A cross-validation method object from scikit-learn.\n               scaler (object, optional): A scaler object from scikit-learn to scale the data.\n               name (str, optional): A name for the cross-validator.\n               pi_kwargs (dict, optional): A dictionary of keyword arguments for permutation importance calculation.\n               pred_probs (bool, optional): Whether to predict probabilities or class labels.\n               perm_imp (dict, optional): A dictionary containing permutation importances for each model.\n               oof_preds (dict, optional): A dictionary containing out-of-fold predictions for each model.\n               oof_metrics (dict, optional): A dictionary containing out-of-fold metric scores for each model.\n               data (tuple, optional): A tuple containing the test features and labels used in each fold.\n               oof_metrics_df (pd.DataFrame, optional): A pandas DataFrame containing the mean out-of-fold metric scores for each model.\n\n           Methods:\n               _calculate_metrics(y_test, y_pred): Calculates and prints the metric scores for a given set of true labels and predicted labels.\n               _cross_validate(X, y): Performs cross-validation on the given data and models, and stores the out-of-fold predictions, metric scores, and permutation importances.\n               _get_oof_metrics(metric_dict): Converts the out-of-fold metric scores dictionary to a pandas DataFrame.\n               fit(X, y): Initiates the cross-validation process and stores the results.\n       \"\"\"\n        \n        self.name = name\n        self.verbose = verbose\n        \n        self.models = models\n        self.metric_fns = metric_fns\n        self.cv_method = cv_method\n        self.scaler = scaler\n        self.pi_kwargs = pi_kwargs\n        self.pred_probs = pred_probs\n        \n        self.perm_imp = None\n        self.oof_preds = None\n        self.oof_metrics = None\n        self.data = None\n        \n        self.oof_metrics_df = None\n    \n    def _calculate_metrics(self, y_test, y_pred):\n        # Dictionary to store the score for each metric\n        results = {}\n\n        # Loop through each metric\n        for metric_fn in self.metric_fns:\n            # Calculate score using metric\n            if metric_fn[0] == 'ROC AUC':\n                score = metric_fn[1](y_test, y_pred, multi_class = 'ovr')\n            else:\n                score = metric_fn[1](y_test, y_pred)\n\n            # Store score as value and metric as key\n            results[metric_fn[0]] = score\n            \n            if self.verbose:\n                # Display metric score\n                print(f'{metric_fn[0]} : {score:.5f}\\n')\n        \n        return results\n\n    def _cross_validate(self, X, y):\n        # Dictionaries to store out-of-fold predictions and out-of-fold metric scores\n        oof_preds, oof_metrics = {}, {}\n        \n        # Lists to aggregate test features and labels used in each fold\n        x_data, y_data = [], []\n        \n        # Dictionary to store permutation feature importance for each fold\n        perm_imp = {}\n        \n        if self.verbose:\n            print(f'Name: {self.name} | {self.cv_method.n_splits}-Fold\\n')\n        \n        for idx, (train_idx, test_idx) in enumerate(self.cv_method.split(X, y)):\n            if self.verbose:\n                print(f'Fold {idx}:')\n                print('-'*40+'\\n')\n            \n            x_train, x_test = X[train_idx], X[test_idx]\n            y_train, y_test = y[train_idx], y[test_idx]\n            \n            x_data.extend(x_test)\n            y_data.extend(y_test)\n            \n            if self.scaler is not None:\n                x_train = self.scaler.fit_transform(x_train)\n                x_test = self.scaler.transform(x_test)\n            \n            for model in self.models:\n                if self.verbose:\n                    print(f'Cross-validating: [{model[0]}]\\n')\n                \n                model[1].fit(x_train, y_train)\n                \n                if model[0] not in oof_preds:\n                    oof_preds[model[0]] = []\n                    oof_metrics[model[0]] = {}                    \n                    perm_imp[model[0]] = []\n                \n                y_pred = model[1].predict_proba(x_test) if self.pred_probs else model[1].predict(x_test)\n                \n                # Save model predictions\n                oof_preds[model[0]].append(y_pred)\n                \n                # Calculate metrics\n                for metric_name, result in self._calculate_metrics(y_test, y_pred).items():\n                    # Save metric result if not already in dictionary\n                    if metric_name not in oof_metrics[model[0]]:\n                        oof_metrics[model[0]][metric_name] = []\n                    oof_metrics[model[0]][metric_name].append(result)\n                \n                # Calculate permutation importances\n                if self.pi_kwargs is not None:\n                    if self.verbose:\n                        print(' -- Calculating Permutation Importances...\\n')\n                    perm_result = permutation_importance(model[1], x_test, y_test, **self.pi_kwargs)\n                    perm_imp[model[0]].append(perm_result.importances)\n        \n        if self.pi_kwargs is not None:\n            self.perm_imp = perm_imp\n        \n        return oof_preds, oof_metrics, (x_data, y_data)\n    \n    def _get_oof_metrics(self, metric_dict):\n        dict, df = {'models' : []}, pd.DataFrame()\n        \n        for model in list(metric_dict.keys()):\n            dict['models'].append(model)\n            for metric in list(metric_dict[model].keys()):\n                if metric not in dict:\n                    dict[metric] = []\n                dict[metric].append(np.mean(metric_dict[model][metric]))\n\n        df.index = dict['models']\n        for metric in list(dict.keys())[1:]:\n            df[metric] = dict[metric]\n\n        return df      \n    \n    def fit(self, X, y):\n        self.oof_preds, self.oof_metrics, self.data = self._cross_validate(X, y)\n        self.oof_metrics_df = self._get_oof_metrics(self.oof_metrics)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_model(\n    model,\n    train_loader, \n    loss_fn,\n    optimizer,\n    metric_fn,\n    device,\n    scheduler=None,\n):\n    model.train()\n    running_metric = 0.0\n    running_loss = 0.0\n\n    with tqdm(train_loader, unit=\"batch\", desc=\"Training\") as progress_bar:\n        for batch, (x, y) in enumerate(progress_bar):\n            x, y = x.to(device), y.to(device)\n\n            optimizer.zero_grad()\n            \n            yhat = model(x) # Model returns logits,\n            yhat = torch.sigmoid(yhat) # Apply sigmoid for pred probs !!! \n\n            loss = loss_fn(yhat, y)\n            loss.backward()\n            optimizer.step()\n\n            metric_fn.update(yhat, y)\n            running_loss += loss.item()\n            running_metric += metric_fn.compute().item()\n\n            progress_bar.set_postfix(loss=loss.item(), \n                                     metric=running_metric / (batch + 1))\n\n    epoch_loss = running_loss / len(train_loader)\n    epoch_metric = running_metric / len(train_loader)\n\n    metric_fn.reset()\n\n    print(f\"\\nTraining Loss: {epoch_loss:.4f}, F1 Score: {epoch_metric:.4f}\")\n    return epoch_loss, epoch_metric\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T05:50:40.435725Z","iopub.execute_input":"2025-04-23T05:50:40.436034Z","iopub.status.idle":"2025-04-23T05:50:40.444586Z","shell.execute_reply.started":"2025-04-23T05:50:40.436013Z","shell.execute_reply":"2025-04-23T05:50:40.443529Z"}},"outputs":[],"execution_count":55},{"cell_type":"code","source":"def validate_model(\n    model,\n    valid_loader,\n    loss_fn,\n    metric_fn,\n    device\n):\n    model.eval()\n    running_loss = 0.0\n    running_metric = 0.0\n\n    with torch.inference_mode():\n        with tqdm(valid_loader, unit=\"batch\", desc=\"Validation\") as progress_bar:\n            for x, y in progress_bar:\n                x, y = x.to(device), y.to(device)\n\n                yhat = model(x)\n                yhat = torch.sigmoid(yhat)\n\n                loss = loss_fn(yhat, y)\n\n                metric_fn.update(yhat, y)\n\n                running_loss += loss.item()\n                running_metric += metric_fn.compute().item()\n\n                progress_bar.set_postfix(loss=loss.item(), \n                                         metric=running_metric / (len(progress_bar) + 1))\n\n    epoch_loss = running_loss / len(valid_loader)\n    epoch_metric = running_metric / len(valid_loader)\n\n    metric_fn.reset()\n\n    print(f\"\\nValidation Loss: {epoch_loss:.4f}, F1 Score: {epoch_metric:.4f}\")\n    return epoch_loss, epoch_metric\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T05:50:40.755902Z","iopub.execute_input":"2025-04-23T05:50:40.756234Z","iopub.status.idle":"2025-04-23T05:50:40.763685Z","shell.execute_reply.started":"2025-04-23T05:50:40.756211Z","shell.execute_reply":"2025-04-23T05:50:40.762719Z"}},"outputs":[],"execution_count":56},{"cell_type":"code","source":"def training_pipeline(\n    epochs,\n    model,\n    train_loader,\n    valid_loader,\n    loss_fn,\n    metric_fn,\n    optimizer,\n    device,\n    scheduler=None,\n):\n    train_losses, valid_losses = [], []\n    train_metrics, valid_metrics = [], []\n    \n    for epoch in range(epochs):\n        print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n\n        train_loss, train_metric = train_model(\n            model, train_loader, loss_fn, optimizer, metric_fn, device, scheduler\n        )\n        valid_loss, valid_metric = validate_model(\n            model, valid_loader, loss_fn, metric_fn, device\n        )\n\n        train_losses.append(train_loss)\n        valid_losses.append(valid_loss)\n        train_metrics.append(train_metric)\n        valid_metrics.append(valid_metric)\n\n        if scheduler:\n            scheduler.step(valid_loss)\n\n        print(f\"Train Loss: {train_loss:.4f}, Train Metric: {train_metric:.4f}\")\n        print(f\"Validation Loss: {valid_loss:.4f}, Validation Metric: {valid_metric:.4f}\")\n\n    return train_losses, valid_losses, train_metrics, valid_metrics","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T05:50:56.813594Z","iopub.execute_input":"2025-04-23T05:50:56.814189Z","iopub.status.idle":"2025-04-23T05:50:56.820536Z","shell.execute_reply.started":"2025-04-23T05:50:56.814162Z","shell.execute_reply":"2025-04-23T05:50:56.819499Z"}},"outputs":[],"execution_count":57},{"cell_type":"code","source":"train_losses, valid_losses, train_metrics, valid_metrics = training_pipeline(\n    10, effnet_model, train_loader, valid_loader, focal_loss, metric_fn, optimizer, device, scheduler\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T05:50:58.769536Z","iopub.execute_input":"2025-04-23T05:50:58.770383Z","iopub.status.idle":"2025-04-23T05:51:40.341625Z","shell.execute_reply.started":"2025-04-23T05:50:58.770352Z","shell.execute_reply":"2025-04-23T05:51:40.340128Z"}},"outputs":[{"name":"stdout","text":"\nEpoch 1/10\n","output_type":"stream"},{"name":"stderr","text":"Training:   0%|          | 5/3978 [00:41<9:09:44,  8.30s/batch, loss=0.269, metric=0] \n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_31/2440156194.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m train_losses, valid_losses, train_metrics, valid_metrics = training_pipeline(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meffnet_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfocal_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m )\n","\u001b[0;32m/tmp/ipykernel_31/3318995164.py\u001b[0m in \u001b[0;36mtraining_pipeline\u001b[0;34m(epochs, model, train_loader, valid_loader, loss_fn, metric_fn, optimizer, device, scheduler)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nEpoch {epoch + 1}/{epochs}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         train_loss, train_metric = train_model(\n\u001b[0m\u001b[1;32m     19\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         )\n","\u001b[0;32m/tmp/ipykernel_31/1096604540.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, loss_fn, optimizer, metric_fn, device, scheduler)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myhat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    579\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             )\n\u001b[0;32m--> 581\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    582\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    826\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":58},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}